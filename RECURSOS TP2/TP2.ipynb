{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3b41b147-695b-447c-860f-c5fb86aa2b97",
   "metadata": {},
   "source": [
    "# Punto 1: Autovectores y autovalores de L y R\n",
    "## Punto a\n",
    "Muestren que el vector de unos $1$ es autovector de las matrices $R$ y $L$. ¿Qué autovalor tiene? ¿Y qué agrupación de la red representa?\n",
    "### Respuesta\n",
    "Considerando la matriz laplaciana $L=K-A$, donde:\n",
    "- $K$ la matriz diagonal de grados del grafo, definida como $K=\\text{diag}(k_1,k_2,...,k_n)$, donde cada $k_i=\\sum_jA_{ij}$ representa el grado del nodo $i$.\n",
    "- $A$ es la matriz de adyacencia del grafo, que asumimos no dirigido y sin lazos, por lo que es simétrica y tiene ceros en la diagonal.\n",
    "Al evaluar la acción de $L$ sobre el vector $1$ tenemos que\n",
    "$$L\\cdot1=(K-A)\\cdot1=K\\cdot1-A\\cdot1$$\n",
    "Dado que $K\\cdot1=(k_1,...,k_n)^t$ y $A\\cdot1=(k_1,...,k_n)^t$, se concluye que:\n",
    "$$L\\cdot1=0$$\n",
    "Por lo tanto, el vector $1$ es autovector de $L$ con autovalor $\\lambda=0$.\n",
    "Este autovector trivial representa la agrupación en la que todos los nodos de la red pertenecen a un único grupo, es decir, **sin partición**.\n",
    "Luego, probamos que $1$ es autovector de $R$ utilizando la matriz de modularidad $R=A-P$ donde $P$ es la matriz que representa las conexiones esperadas entre nodos bajo el **modelo de configuración**, y se define como:\n",
    "$$P_{ij}=\\frac{k_ik_j}{2E}$$\n",
    "donde $E=\\frac{1}{2}\\sum_{i,j}A_{ij}$ es el número total de enlaces en la red. Entonces, el producto de $P$ por el vector $1$ da:\n",
    "$$(P\\cdot1)_i=\\sum_j\\frac{k_ik_j}{2E}=\\frac{k_i}{2E}\\sum_jk_j=\\frac{k_i\\cdot2E}{2E}=k_i$$\n",
    "Dado que $A\\cdot1=(k_1,...,k_n)^t$, tenemos que:\n",
    "$$R\\cdot1=(A-P)\\cdot1=A\\cdot1-P\\cdot1=(k_1,...,k_n)^t-(k_1,...,k_n)^t=0$$\n",
    "Por lo tanto, el vector $1$ también es autovector de la matriz $R$, con autovalor $\\lambda=0$.\n",
    "Tal como en el caso anterior, este autovector representa la no-partición: una configuración en la que todos los nodos de la red son asignados al mismo grupo.\n",
    "## Punto b\n",
    "Muestren que si $L(R)$ tienen dos autovectores $v_1$ y $v_2$ asociados a autovalores $\\lambda_1\\neq\\lambda_2$, entonces $v_1^tv_2=0$. *Tip: Consideren una matriz $M$ simétrica con dos autovectores $v_1$ y $v_2$ con autovalores distintos $\\lambda_1$ y $\\lambda_2$. Comparen el resultado de hacer $v_1^tMv_2$ y $v_2^tMv_1$.*\n",
    "### Respuesta\n",
    "Sea $M\\in\\mathbb{R}^{n\\times n}$ una matriz simétrica (al igual que son $L$ o $R$). Supongamos que $v_1$ y $v_2$ son autovectores de $M$ asociados a autovalores $\\lambda_1$ y $\\lambda_2$ respectivamente, con $\\lambda_1\\neq\\lambda_2$. Se quiere demostrar que $v_1^tv_2=0$.\n",
    "Dado que $v_1$ y $v_2$ son autovectores, se cumple que\n",
    "$$Mv_1=\\lambda_1v_1,\\quad Mv_2=\\lambda_2v_2$$\n",
    "Multiplicando escalarmente la primera ecuación por $v_2^t$ tenemos que:\n",
    "$$v_2^tMv_1=\\lambda_1v_2^tv_1$$\n",
    "Análogamente, al multiplicar la segunda ecuación por $v_1^t$ obtenemos\n",
    "$$v_1^tMv_2=\\lambda_2v_1^tv_2$$\n",
    "Ahora bien, utilizando la simetría de $M$, se tiene que:\n",
    "$$v_2^tMv_1=v_1^tMv_2$$\n",
    "Por lo tanto, las dos expresiones anteriores implican que\n",
    "$$\\lambda_1v_2^tv_1=\\lambda_2v_1^tv_2$$\n",
    "Dado que el producto escalar es simétrico, es decir $v_1^tv_2=v_2^tv_1$, se obtiene que\n",
    "$$\\lambda_1v_1^tv_2=\\lambda_2v_1^tv_2$$\n",
    "De lo cual deducimos que\n",
    "$$(\\lambda_1-\\lambda_2)v_1^tv_2=0$$\n",
    "Como inicialmente asumimos que $\\lambda_1\\neq\\lambda_2$, se concluye que $v_1^tv_2=0$. Por lo tanto, los autovectores correspondientes a autovalores distintos de una matriz simétrica son ortogonales. En particular, esto se cumple para las matrices $L$ y $R$, ya que ambas son simétricas.\n",
    "## Punto c\n",
    "Muestren si $v$ es un autovector de autovalor $\\lambda\\neq0$ de $R$ o $L$, entonces $\\sum_iv_i=0$.\n",
    "### Respuesta\n",
    "Sea $v\\in\\mathbb{R}^n$ un autovector asociado al autovalor $\\lambda=0$ de una de las matrices simétricas consideradas, $L$ o $R$. Se busca determinar si necesariamente se cumple que $\\sum_{i=1}^{n}v_i=0$.\n",
    "Primero consideramos la matriz laplaciana $L=K−A$. Como se demostró previamente, el vector $1=(1,\\dots,1)^t$ es autovector de $L$ con autovalor $\\lambda=0$. Si la red es conexa, el subespacio nulo de $L$ es unidimensional, por lo que cualquier autovector asociado a $\\lambda=0$ es múltiplo escalar de $1$. En tal caso, su suma es $\\sum_iv_i=c\\cdot n$, con $c\\neq0$, y por lo tanto:\n",
    "$$\\sum_{i=1}^nv_i\\neq0$$\n",
    "Luego, la afirmación resulta falsa para $L$ si se interpreta como una propiedad general: existen autovectores con $\\lambda=0$ cuya suma no es nula.\n",
    "En el caso de la matriz de modularidad $R=A−P$, también se tiene que $R\\cdot1=0$, por lo que $1$ es nuevamente un autovector asociado a $\\lambda=0$, con suma no nula. Sin embargo, a diferencia de la matriz laplaciana, la matriz $R$ puede presentar un núcleo de dimensión mayor a uno, dependiendo de la estructura de la red. En este caso, pueden existir autovectores $v\\in\\ker(R)$ tales que $\\sum_iv_i=0$.\n",
    "Es decir, si $v\\in\\ker(R)$ y $v\\perp1$, entonces necesariamente $\\sum_iv_i=1^t v=0$. De hecho, en aplicaciones prácticas, tales autovectores ortogonales a $1$ son útiles en tareas de partición espectral, ya que codifican divisiones balanceadas entre grupos."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9a13b86-07fb-4e27-a524-b1d523376e52",
   "metadata": {},
   "source": [
    "# Ejercicio 2: Extensiones del método de la potencia\n",
    "Consideren una matriz $M\\in\\mathbb{R}^{n\\times n}$ diagonalizable con autovalores $\\lambda_1\\geq\\lambda_2\\geq\\cdots\\geq\\lambda_n$, y autovector $v_i$ asociado a $\\lambda_i$.\n",
    "## Punto a: Shifting de autovalores\n",
    "Muestre que los autovalores de $M+\\mu I$ son $\\gamma_i=\\lambda_i+\\mu$, y que el autovector asociado a $\\gamma_i$ es $v_i$. Concluya que si $\\mu+\\lambda_i\\neq0,\\forall i$, entonces $M+\\mu I$ es inversible.\n",
    "### Respuesta\n",
    "## Punto b: Método de la potencia inverso\n",
    "Considerando $\\mu>0$, muestren que $L+\\mu I$ es inversible, con $L$ el laplaciano definido como $L=K-A$. Muestren que aplicar el método de la potencia a $(L+\\mu I)^{-1}$ converge a su autovector de autovalor más chico si se parte de una semilla adecuada. Indique, en el caso de que hay sólo un autovector con dicho autovalor, cuál es dicho autovector y cuánto vale su autovalor.\n",
    "### Respuesta\n",
    "## Punto c: Deflación de Hotelling\n",
    "Suponiendo que $M$ es simétrica (y por lo tanto admite una base ortogonal de autovectores), muestre que la matriz $\\tilde{M}-\\lambda_1\\frac{v_1v_1^t}{v_1^tv_1}$ tiene los mismos autovectores que $M$, pero el autovalor asociado a $v_1$ es igual a cero.\n",
    "### Respuesta"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d31e12f7-40ec-4b57-a8ef-47432cf748c8",
   "metadata": {},
   "source": [
    "# Ejercicio 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6aa745e-460d-4d58-9283-a0174ba44ea9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargamos librerías\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.linalg import solve_triangular\n",
    "\n",
    "# Cargamos funciones auxiliares\n",
    "\n",
    "# Función para permutar filas (para descompPLU)\n",
    "def permutacion(A, vector_P, index):\n",
    "    n = A.shape[0]\n",
    "    max_index = index + np.argmax(np.abs(A[index:, index]))\n",
    "    #swap\n",
    "    if max_index != index:\n",
    "        A[[index, max_index]] = A[[max_index, index]]\n",
    "        vector_P[[index, max_index]] = vector_P[[max_index, index]]\n",
    "\n",
    "\n",
    "# Descomposición PLU con pivoteo\n",
    "def calculaPLU(m, verbose=False):\n",
    "    mc = m.copy().astype(np.float64)\n",
    "    n = m.shape[0]\n",
    "    P = np.eye(n)\n",
    "    for i in range(n - 1):\n",
    "        max_row = i + np.argmax(np.abs(mc[i:, i]))\n",
    "        if max_row != i:\n",
    "            mc[[i, max_row]] = mc[[max_row, i]]\n",
    "            P[[i, max_row]] = P[[max_row, i]]\n",
    "        a_ii = mc[i, i]\n",
    "        if a_ii == 0:\n",
    "            raise ValueError(\"Matriz singular (no invertible)\")\n",
    "        L_i = mc[i+1:, i] / a_ii\n",
    "        mc[i+1:, i] = L_i\n",
    "        mc[i+1:, i+1:] -= np.outer(L_i, mc[i, i+1:])\n",
    "    \n",
    "    L = np.tril(mc, -1) + np.eye(n)\n",
    "    U = np.triu(mc)\n",
    "    if verbose:\n",
    "        print(\"P:\\n\", P)\n",
    "        print(\"L:\\n\", L)\n",
    "        print(\"U:\\n\", U)\n",
    "    return P, L, U\n",
    "\n",
    "\n",
    "def calculaLU(matriz, verbose=False):\n",
    "    mc = matriz.copy().astype(np.float64)\n",
    "    n = matriz.shape[0]\n",
    "    for i in range(n - 1):\n",
    "        a_ii = mc[i, i]\n",
    "        if a_ii == 0:\n",
    "            raise ValueError(\"Cero en la diagonal durante LU (se requiere pivoteo)\")\n",
    "        L_i = mc[i+1:, i] / a_ii\n",
    "        mc[i+1:, i] = L_i\n",
    "        mc[i+1:, i+1:] -= np.outer(L_i, mc[i, i+1:])\n",
    "    \n",
    "    L = np.tril(mc, -1) + np.eye(n)\n",
    "    U = np.triu(mc)\n",
    "    if verbose:\n",
    "        print(\"L:\\n\", L)\n",
    "        print(\"U:\\n\", U)\n",
    "    return L, U\n",
    "\n",
    "# Función para calcular la inversa corregida\n",
    "def inversa(m):\n",
    "    n = m.shape[0]\n",
    "    try:\n",
    "        L, U = calculaLU(m)\n",
    "        P = np.eye(n)  # Matriz de permutación identidad si no hay pivoteo\n",
    "    except (ValueError, np.LinAlgError):\n",
    "        P, L, U = calculaPLU(m)\n",
    "    \n",
    "    m_inv = np.zeros((n, n))\n",
    "    for i in range(n):\n",
    "        e_i = P.T @ np.eye(n)[:, i]  # Aplica la permutación P al vector canónico\n",
    "        y = solve_triangular(L, e_i, lower=True)\n",
    "        x = solve_triangular(U, y, lower=False)\n",
    "        m_inv[:, i] = x\n",
    "    return m_inv\n",
    "\n",
    "def calcula_K (A):\n",
    "    n = A.shape[0]\n",
    "    k = np.zeros((n,n),dtype=A.dtype)\n",
    "    for i in range(n):\n",
    "        k[i][i] = A[i].sum()\n",
    "    \n",
    "    return k\n",
    "\n",
    "def norma2(v):\n",
    "    n = 0\n",
    "    for k in v:\n",
    "        n+=k*k\n",
    "    return np.sqrt(n)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "137b7480-459b-4c26-b9b3-0784129d4597",
   "metadata": {},
   "source": [
    "## a. Construccion de matrices\n",
    "\n",
    "### i. Matrices L y R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1761813f-8714-4c15-8634-83e32bd839fb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c08f74f9-5139-4c3c-b6dc-422d17e3c93a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "271bc17b-1396-4cd2-a9c3-b2ffb63baed6",
   "metadata": {},
   "source": [
    "##  c. Funciones que realizan particiones iterativas de los grafos\n",
    "\n",
    "### i. Funcion para calcular el Laplaciano de forma iterativa (corte mínimo). \n",
    "Sea una red sin dirigir representada en un grafo al cual le asociamos la matriz  $A \\in \\mathbb{R}^{N \\times N}$ para este grafo el cual representa $A_{ij}= 1$ si $i$ y $j$ están conectados y $A_{ij}= 0$ si no.  \n",
    "Podemos detectar _grupos_ en esta red. Por ejemplo para el caso más básico; queremos identificar dos grupos $G_1, G_2$. Representamos la asignación en comunidades mediante el vector **s** definido $s_i =1$ si $i \\in G_1$ y $s_i =-1$ si $i \\in G_2$. Notamos la cantidad de conexiones entre ambos grupos por $\\Lambda$\n",
    "$$\\Lambda = \\frac{1}{4} s^tLs  \\quad \\text{con } L= K -A $$\n",
    "Buscamos un vector $s^{\\Lambda}$ optimo que minimice $\\Lambda$, para ello buscamos el segundo autovector de autovalor más pequeño de la matriz _L_ . Si aplicamos el mètodo de la potencia a $L^{-1}$ el autovalor de menor valor absoluto pasa a ser el dominante. En general, aplicando el método a $(L- \\mu I)^{-1}$ con $\\mu$ cercano a $\\lambda_{N-1}$ conseguiremos que $\\lambda_{N-1}$ sea dominante"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad2ddc69-be5f-4b44-8aea-9bcecb2f788a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddf3e256-5ff2-4180-abd2-64f72e523170",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
