{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3b41b147-695b-447c-860f-c5fb86aa2b97",
   "metadata": {},
   "source": [
    "# Punto 1: Autovectores y autovalores de L y R\n",
    "## Punto a\n",
    "Muestren que el vector de unos $1$ es autovector de las matrices $R$ y $L$. ¿Qué autovalor tiene? ¿Y qué agrupación de la red representa?\n",
    "### Respuesta\n",
    "Considerando la matriz laplaciana $L=K-A$, donde:\n",
    "- $K$ la matriz diagonal de grados del grafo, definida como $K=\\text{diag}(k_1,k_2,...,k_n)$, donde cada $k_i=\\sum_jA_{ij}$ representa el grado del nodo $i$.\n",
    "- $A$ es la matriz de adyacencia del grafo, que asumimos no dirigido y sin lazos, por lo que es simétrica y tiene ceros en la diagonal.\n",
    "Al evaluar la acción de $L$ sobre el vector $1$ tenemos que\n",
    "$$L\\cdot1=(K-A)\\cdot1=K\\cdot1-A\\cdot1$$\n",
    "Dado que $K\\cdot1=(k_1,...,k_n)^t$ y $A\\cdot1=(k_1,...,k_n)^t$, se concluye que:\n",
    "$$L\\cdot1=0$$\n",
    "Por lo tanto, el vector $1$ es autovector de $L$ con autovalor $\\lambda=0$.\n",
    "Este autovector trivial representa la agrupación en la que todos los nodos de la red pertenecen a un único grupo, es decir, **sin partición**.\n",
    "Luego, probamos que $1$ es autovector de $R$ utilizando la matriz de modularidad $R=A-P$ donde $P$ es la matriz que representa las conexiones esperadas entre nodos bajo el **modelo de configuración**, y se define como:\n",
    "$$P_{ij}=\\frac{k_ik_j}{2E}$$\n",
    "donde $E=\\frac{1}{2}\\sum_{i,j}A_{ij}$ es el número total de enlaces en la red. Entonces, el producto de $P$ por el vector $1$ da:\n",
    "$$(P\\cdot1)_i=\\sum_j\\frac{k_ik_j}{2E}=\\frac{k_i}{2E}\\sum_jk_j=\\frac{k_i\\cdot2E}{2E}=k_i$$\n",
    "Dado que $A\\cdot1=(k_1,...,k_n)^t$, tenemos que:\n",
    "$$R\\cdot1=(A-P)\\cdot1=A\\cdot1-P\\cdot1=(k_1,...,k_n)^t-(k_1,...,k_n)^t=0$$\n",
    "Por lo tanto, el vector $1$ también es autovector de la matriz $R$, con autovalor $\\lambda=0$.\n",
    "Tal como en el caso anterior, este autovector representa la no-partición: una configuración en la que todos los nodos de la red son asignados al mismo grupo.\n",
    "## Punto b\n",
    "Muestren que si $L(R)$ tienen dos autovectores $v_1$ y $v_2$ asociados a autovalores $\\lambda_1\\neq\\lambda_2$, entonces $v_1^tv_2=0$. *Tip: Consideren una matriz $M$ simétrica con dos autovectores $v_1$ y $v_2$ con autovalores distintos $\\lambda_1$ y $\\lambda_2$. Comparen el resultado de hacer $v_1^tMv_2$ y $v_2^tMv_1$.*\n",
    "### Respuesta\n",
    "Sea $M\\in\\mathbb{R}^{n\\times n}$ una matriz simétrica (al igual que son $L$ o $R$). Supongamos que $v_1$ y $v_2$ son autovectores de $M$ asociados a autovalores $\\lambda_1$ y $\\lambda_2$ respectivamente, con $\\lambda_1\\neq\\lambda_2$. Se quiere demostrar que $v_1^tv_2=0$.\n",
    "Dado que $v_1$ y $v_2$ son autovectores, se cumple que\n",
    "$$Mv_1=\\lambda_1v_1,\\quad Mv_2=\\lambda_2v_2$$\n",
    "Multiplicando escalarmente la primera ecuación por $v_2^t$ tenemos que:\n",
    "$$v_2^tMv_1=\\lambda_1v_2^tv_1$$\n",
    "Análogamente, al multiplicar la segunda ecuación por $v_1^t$ obtenemos\n",
    "$$v_1^tMv_2=\\lambda_2v_1^tv_2$$\n",
    "Ahora bien, utilizando la simetría de $M$, se tiene que:\n",
    "$$v_2^tMv_1=v_1^tMv_2$$\n",
    "Por lo tanto, las dos expresiones anteriores implican que\n",
    "$$\\lambda_1v_2^tv_1=\\lambda_2v_1^tv_2$$\n",
    "Dado que el producto escalar es simétrico, es decir $v_1^tv_2=v_2^tv_1$, se obtiene que\n",
    "$$\\lambda_1v_1^tv_2=\\lambda_2v_1^tv_2$$\n",
    "De lo cual deducimos que\n",
    "$$(\\lambda_1-\\lambda_2)v_1^tv_2=0$$\n",
    "Como inicialmente asumimos que $\\lambda_1\\neq\\lambda_2$, se concluye que $v_1^tv_2=0$. Por lo tanto, los autovectores correspondientes a autovalores distintos de una matriz simétrica son ortogonales. En particular, esto se cumple para las matrices $L$ y $R$, ya que ambas son simétricas.\n",
    "## Punto c\n",
    "Muestren si $v$ es un autovector de autovalor $\\lambda\\neq0$ de $R$ o $L$, entonces $\\sum_iv_i=0$.\n",
    "### Respuesta\n",
    "Sea $v\\in\\mathbb{R}^n$ un autovector asociado al autovalor $\\lambda=0$ de una de las matrices simétricas consideradas, $L$ o $R$. Se busca determinar si necesariamente se cumple que $\\sum_{i=1}^{n}v_i=0$.\n",
    "Primero consideramos la matriz laplaciana $L=K−A$. Como se demostró previamente, el vector $1=(1,\\dots,1)^t$ es autovector de $L$ con autovalor $\\lambda=0$. Si la red es conexa, el subespacio nulo de $L$ es unidimensional, por lo que cualquier autovector asociado a $\\lambda=0$ es múltiplo escalar de $1$. En tal caso, su suma es $\\sum_iv_i=c\\cdot n$, con $c\\neq0$, y por lo tanto:\n",
    "$$\\sum_{i=1}^nv_i\\neq0$$\n",
    "Luego, la afirmación resulta falsa para $L$ si se interpreta como una propiedad general: existen autovectores con $\\lambda=0$ cuya suma no es nula.\n",
    "En el caso de la matriz de modularidad $R=A−P$, también se tiene que $R\\cdot1=0$, por lo que $1$ es nuevamente un autovector asociado a $\\lambda=0$, con suma no nula. Sin embargo, a diferencia de la matriz laplaciana, la matriz $R$ puede presentar un núcleo de dimensión mayor a uno, dependiendo de la estructura de la red. En este caso, pueden existir autovectores $v\\in\\ker(R)$ tales que $\\sum_iv_i=0$.\n",
    "Es decir, si $v\\in\\ker(R)$ y $v\\perp1$, entonces necesariamente $\\sum_iv_i=1^t v=0$. De hecho, en aplicaciones prácticas, tales autovectores ortogonales a $1$ son útiles en tareas de partición espectral, ya que codifican divisiones balanceadas entre grupos."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9a13b86-07fb-4e27-a524-b1d523376e52",
   "metadata": {},
   "source": [
    "# Ejercicio 2: Extensiones del método de la potencia\n",
    "Consideren una matriz $M\\in\\mathbb{R}^{n\\times n}$ diagonalizable con autovalores $\\lambda_1\\geq\\lambda_2\\geq\\cdots\\geq\\lambda_n$, y autovector $v_i$ asociado a $\\lambda_i$.\n",
    "## Punto a: Shifting de autovalores\n",
    "Muestre que los autovalores de $M+\\mu I$ son $\\gamma_i=\\lambda_i+\\mu$, y que el autovector asociado a $\\gamma_i$ es $v_i$. Concluya que si $\\mu+\\lambda_i\\neq0,\\forall i$, entonces $M+\\mu I$ es inversible.\n",
    "### Respuesta\n",
    "## Punto b: Método de la potencia inverso\n",
    "Considerando $\\mu>0$, muestren que $L+\\mu I$ es inversible, con $L$ el laplaciano definido como $L=K-A$. Muestren que aplicar el método de la potencia a $(L+\\mu I)^{-1}$ converge a su autovector de autovalor más chico si se parte de una semilla adecuada. Indique, en el caso de que hay sólo un autovector con dicho autovalor, cuál es dicho autovector y cuánto vale su autovalor.\n",
    "### Respuesta\n",
    "## Punto c: Deflación de Hotelling\n",
    "Suponiendo que $M$ es simétrica (y por lo tanto admite una base ortogonal de autovectores), muestre que la matriz $\\tilde{M}-\\lambda_1\\frac{v_1v_1^t}{v_1^tv_1}$ tiene los mismos autovectores que $M$, pero el autovalor asociado a $v_1$ es igual a cero.\n",
    "### Respuesta"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
