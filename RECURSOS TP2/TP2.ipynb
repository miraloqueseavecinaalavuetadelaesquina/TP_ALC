{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3b41b147-695b-447c-860f-c5fb86aa2b97",
   "metadata": {},
   "source": [
    "# Punto 1: Autovectores y autovalores de L y R\n",
    "## Punto a\n",
    "Muestren que el vector de unos $1$ es autovector de las matrices $R$ y $L$. ¿Qué autovalor tiene? ¿Y qué agrupación de la red representa?\n",
    "### Respuesta\n",
    "Considerando la matriz laplaciana $L=K-A$, donde:\n",
    "- $K$ la matriz diagonal de grados del grafo, definida como $K=\\text{diag}(k_1,k_2,...,k_n)$, donde cada $k_i=\\sum_jA_{ij}$ representa el grado del nodo $i$.\n",
    "- $A$ es la matriz de adyacencia del grafo, que asumimos no dirigido y sin lazos, por lo que es simétrica y tiene ceros en la diagonal.\n",
    "Al evaluar la acción de $L$ sobre el vector $1$ tenemos que\n",
    "$$L\\cdot1=(K-A)\\cdot1=K\\cdot1-A\\cdot1$$\n",
    "Dado que $K\\cdot1=(k_1,...,k_n)^t$ y $A\\cdot1=(k_1,...,k_n)^t$, se concluye que:\n",
    "$$L\\cdot1=0$$\n",
    "Por lo tanto, el vector $1$ es autovector de $L$ con autovalor $\\lambda=0$.\n",
    "Este autovector trivial representa la agrupación en la que todos los nodos de la red pertenecen a un único grupo, es decir, **sin partición**.\n",
    "Luego, probamos que $1$ es autovector de $R$ utilizando la matriz de modularidad $R=A-P$ donde $P$ es la matriz que representa las conexiones esperadas entre nodos bajo el **modelo de configuración**, y se define como:\n",
    "$$P_{ij}=\\frac{k_ik_j}{2E}$$\n",
    "donde $E=\\frac{1}{2}\\sum_{i,j}A_{ij}$ es el número total de enlaces en la red. Entonces, el producto de $P$ por el vector $1$ da:\n",
    "$$(P\\cdot1)_i=\\sum_j\\frac{k_ik_j}{2E}=\\frac{k_i}{2E}\\sum_jk_j=\\frac{k_i\\cdot2E}{2E}=k_i$$\n",
    "Dado que $A\\cdot1=(k_1,...,k_n)^t$, tenemos que:\n",
    "$$R\\cdot1=(A-P)\\cdot1=A\\cdot1-P\\cdot1=(k_1,...,k_n)^t-(k_1,...,k_n)^t=0$$\n",
    "Por lo tanto, el vector $1$ también es autovector de la matriz $R$, con autovalor $\\lambda=0$.\n",
    "Tal como en el caso anterior, este autovector representa la no-partición: una configuración en la que todos los nodos de la red son asignados al mismo grupo.\n",
    "## Punto b\n",
    "Muestren que si $L(R)$ tienen dos autovectores $v_1$ y $v_2$ asociados a autovalores $\\lambda_1\\neq\\lambda_2$, entonces $v_1^tv_2=0$. *Tip: Consideren una matriz $M$ simétrica con dos autovectores $v_1$ y $v_2$ con autovalores distintos $\\lambda_1$ y $\\lambda_2$. Comparen el resultado de hacer $v_1^tMv_2$ y $v_2^tMv_1$.*\n",
    "### Respuesta\n",
    "Sea $M\\in\\mathbb{R}^{n\\times n}$ una matriz simétrica (al igual que son $L$ o $R$). Supongamos que $v_1$ y $v_2$ son autovectores de $M$ asociados a autovalores $\\lambda_1$ y $\\lambda_2$ respectivamente, con $\\lambda_1\\neq\\lambda_2$. Se quiere demostrar que $v_1^tv_2=0$.\n",
    "Dado que $v_1$ y $v_2$ son autovectores, se cumple que\n",
    "$$Mv_1=\\lambda_1v_1,\\quad Mv_2=\\lambda_2v_2$$\n",
    "Multiplicando escalarmente la primera ecuación por $v_2^t$ tenemos que:\n",
    "$$v_2^tMv_1=\\lambda_1v_2^tv_1$$\n",
    "Análogamente, al multiplicar la segunda ecuación por $v_1^t$ obtenemos\n",
    "$$v_1^tMv_2=\\lambda_2v_1^tv_2$$\n",
    "Ahora bien, utilizando la simetría de $M$, se tiene que:\n",
    "$$v_2^tMv_1=v_1^tMv_2$$\n",
    "Por lo tanto, las dos expresiones anteriores implican que\n",
    "$$\\lambda_1v_2^tv_1=\\lambda_2v_1^tv_2$$\n",
    "Dado que el producto escalar es simétrico, es decir $v_1^tv_2=v_2^tv_1$, se obtiene que\n",
    "$$\\lambda_1v_1^tv_2=\\lambda_2v_1^tv_2$$\n",
    "De lo cual deducimos que\n",
    "$$(\\lambda_1-\\lambda_2)v_1^tv_2=0$$\n",
    "Como inicialmente asumimos que $\\lambda_1\\neq\\lambda_2$, se concluye que $v_1^tv_2=0$. Por lo tanto, los autovectores correspondientes a autovalores distintos de una matriz simétrica son ortogonales. En particular, esto se cumple para las matrices $L$ y $R$, ya que ambas son simétricas.\n",
    "## Punto c\n",
    "Muestren si $v$ es un autovector de autovalor $\\lambda\\neq0$ de $R$ o $L$, entonces $\\sum_iv_i=0$.\n",
    "### Respuesta\n",
    "Sea $v\\in\\mathbb{R}^n$ un autovector asociado al autovalor $\\lambda=0$ de una de las matrices simétricas consideradas, $L$ o $R$. Se busca determinar si necesariamente se cumple que $\\sum_{i=1}^{n}v_i=0$.\n",
    "Primero consideramos la matriz laplaciana $L=K−A$. Como se demostró previamente, el vector $1=(1,\\dots,1)^t$ es autovector de $L$ con autovalor $\\lambda=0$. Si la red es conexa, el subespacio nulo de $L$ es unidimensional, por lo que cualquier autovector asociado a $\\lambda=0$ es múltiplo escalar de $1$. En tal caso, su suma es $\\sum_iv_i=c\\cdot n$, con $c\\neq0$, y por lo tanto:\n",
    "$$\\sum_{i=1}^nv_i\\neq0$$\n",
    "Luego, la afirmación resulta falsa para $L$ si se interpreta como una propiedad general: existen autovectores con $\\lambda=0$ cuya suma no es nula.\n",
    "En el caso de la matriz de modularidad $R=A−P$, también se tiene que $R\\cdot1=0$, por lo que $1$ es nuevamente un autovector asociado a $\\lambda=0$, con suma no nula. Sin embargo, a diferencia de la matriz laplaciana, la matriz $R$ puede presentar un núcleo de dimensión mayor a uno, dependiendo de la estructura de la red. En este caso, pueden existir autovectores $v\\in\\ker(R)$ tales que $\\sum_iv_i=0$.\n",
    "Es decir, si $v\\in\\ker(R)$ y $v\\perp1$, entonces necesariamente $\\sum_iv_i=1^t v=0$. De hecho, en aplicaciones prácticas, tales autovectores ortogonales a $1$ son útiles en tareas de partición espectral, ya que codifican divisiones balanceadas entre grupos."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9a13b86-07fb-4e27-a524-b1d523376e52",
   "metadata": {},
   "source": [
    "# Ejercicio 2: Extensiones del método de la potencia\n",
    "Consideren una matriz $M\\in\\mathbb{R}^{n\\times n}$ diagonalizable con autovalores $\\lambda_1\\geq\\lambda_2\\geq\\cdots\\geq\\lambda_n$, y autovector $v_i$ asociado a $\\lambda_i$.\n",
    "## Punto a: Shifting de autovalores\n",
    "Muestre que los autovalores de $M+\\mu I$ son $\\gamma_i=\\lambda_i+\\mu$, y que el autovector asociado a $\\gamma_i$ es $v_i$. Concluya que si $\\mu+\\lambda_i\\neq0,\\forall i$, entonces $M+\\mu I$ es inversible.\n",
    "### Respuesta\n",
    "Si definimos la matriz $M_\\mu=M+\\mu I,\\mu\\in\\mathbb{R}$ tenemos que\n",
    "$$M_\\mu v_i=(M+\\mu I)v_i)=Mv_i+\\mu v_i=(\\lambda_i+\\mu)v_i$$\n",
    "Por lo tanto, los autovalores de $M_\\mu$ son\n",
    "$$\\gamma_i=\\lambda_i+\\mu,\\quad1\\leq i\\leq n$$\n",
    "y cada $v_i$ sigue siendo su autovector asociado.\n",
    "Además, si consideramos que $\\mu+\\lambda_i\\neq 0,\\forall i$, entonces $\\gamma_i\\neq0,\\forall i$. En consecuencia\n",
    "$$\\det(M_\\mu)=\\prod_{i=1}^n\\gamma_i\\neq0$$\n",
    "En conclusión, $M_\\mu$ es inversible.\n",
    "## Punto b: Método de la potencia inverso\n",
    "Considerando $\\mu>0$, muestren que $L+\\mu I$ es inversible, con $L$ el laplaciano definido como $L=K-A$. Muestren que aplicar el método de la potencia a $(L+\\mu I)^{-1}$ converge a su autovector de autovalor más chico si se parte de una semilla adecuada. Indique, en el caso de que hay sólo un autovector con dicho autovalor, cuál es dicho autovector y cuánto vale su autovalor.\n",
    "### Respuesta\n",
    "Analizando la matriz laplaciana $L=K-A$, donde $K$ es la matriz diagonal de grados y $A$ la matriz de adyacencia de un grafo no dirigido y sin lazos, podemos determinar que $L$ es simétrica, pues ambas matrices que la componen lo son. Entonces, tenemos que sus autovalores son reales, pero debemos determinar que dicha matriz resulte semidefinida positiva para abordar el análisis de inversibilidad de la matriz $L_\\mu=L+\\mu I$.\n",
    "#### $L$ es semidefinida positiva\n",
    "Para demostrar que $L$ es semidefinida positiva, debemos demostrar que para todo vector $x\\in\\mathbb{R}^n$, se cumple la desigualdad $x^tLx\\geq0$. Entonces, partiendo de esta expresión y la definición de $L$ tenemos que\n",
    "$$x^tLx=x^t(K-A)x=X^tKx-x^tAx$$\n",
    "La primera parte puede desarrollarse utilizando que $K$ es diagonal\n",
    "$$x^tKx=\\sum_{i=1}^nk_ix_i^2$$\n",
    "donde $k_i\\sum_jA_{ij}$ es el grado del nodo $i$. Luego, siendo $A$ simétrica, obtenemos la siguiente igualdad para el segundo término:\n",
    "$$x^tAx=\\sum_{i=1}^n\\sum_{j=1}^nA_{ij}x_ix_j$$\n",
    "Sustituyendo las expresiones halladas obtenemos\n",
    "$$x^tLx=\\sum_{i=1}^nk_ix_i^2-\\sum_{i=1}^n\\sum_{j=1}^nA_{ij}x_ix_j$$\n",
    "Como $k_i=\\sum_j A_{ij}$, podemos reescribir el primer término como\n",
    "$$x^tLx=\\sum_{i=1}^n\\sum_{j=1}^nA_{ij}x_i^2-\\sum_{i=1}^n\\sum_{j=1}^nA_{ij}x_ix_j$$\n",
    "Y como $A_{ij}$ es un factor común en cada término tenemos que\n",
    "$$x^tLx=\\sum_{i=1}^n\\sum_{j=1}^nA_{ij}(x_i^2-x_ix_j)$$\n",
    "Como la matriz de adyacencia es simétrica y $A_{ij}=A_{ji}$, reescribimos la expresión de manera simétrica, intercambiando índices y promediando ambos términos.\n",
    "$$x^tLx=\\frac{1}{2}\\sum_{i=1}^n\\sum_{j=1}^nA_{ij}(x_i-x_j)^2$$\n",
    "Esto es posible pues\n",
    "$$(x_i-x_j)^2=x_i^2-2x_ix_j+x_j^2=(x_i^2-x_ix_j)+(x_j^2-x_jx_i)$$\n",
    "y dada la simetría de $A_{ij}$ se garantiza que la suma sobre $i,j$ y sobre $j,i$ son iguales. De modo que se obtiene la expresión dada anteriormente para $x^tLx$.\n",
    "Como cada término de la suma es no negativo (ya que los términos están compuesto por un cuadrado, y los pesos $A_{ij}\\geq0$ por definición), la sumatoria es no negativa para todo $x$, demostrando que $L$ es una matriz semidefinida positiva. Por lo tanto, concluimos que\n",
    "$$x^tLx\\geq0,\\forall x\\in\\mathbb{R}^n$$\n",
    "#### Análisis de $L_\\mu$\n",
    "Puesto que $L$ es semidefinida positiva, tenemos que $\\lambda_i(L)\\geq0,1\\leq i\\leq n$. En particular, como el grafo es conexo, el autovalor nulo es simple y tiene como único autovector al vector $1$.\n",
    "Ahora, considerando $\\mu>0$, y el resultado obtenido en el punto a del presente ejercicio, deducimos que los autovalores de $L_\\mu$ son $\\lambda_i+\\mu$, todos estrictamente positivos dada la suposición inicial sobre $\\mu$. Por lo tanto, tenemos que\n",
    "$$\\det(L_\\mu)=\\prod_{i=1}^n(\\lambda_i+\\mu)\\neq0$$\n",
    "En conclusión, $L_\\mu$ resulta inversible.\n",
    "Definiendo $B=L_\\mu^{-1}$, como tenemos que la misma base ortonormal de autovectores $\\{w_i\\}$ de $L$ diagonaliza a $B$, pero ahora con autovalores $\\rho_i=\\frac{1}{\\lambda_i+\\mu}$. Considerando que $\\lambda_n=0$, el mayor autovalor de $B$ resulta ser $\\rho_n=\\frac{1}{\\mu}$ y está asociado al autovector $w_n=\\frac{1}{\\sqrt{n}}$. De este modo, el resto de autovalores $\\rho_i<\\rho_n$.\n",
    "El método de la potencia aplicado a una matriz real simétrica converge hacia el autovector que corresponde al autovalor de mayor módulo siempre que el vector inicial posea proyección distinta de cero sobre dicha dirección. En este caso, escoger cualquier $x^{(0)}\\not\\!\\perp1$ garantiza que la sucesión iterativa $x^{(k+1)}=\\frac{Bx^{(k)}}{\\|Bx^{(k)}\\|_2}$ tiende a $\\frac{1}{\\sqrt{n}}$​. De este modo, la razón de convergencia es $\\frac{\\rho_{n-1}}{\\rho_n}=\\frac{\\mu}{\\lambda_{n-1}+\\mu}<1$. Así, la potencia inversa sobre $L_\\mu$​ identifica el autovector correspondiente al autovalor mínimo de $L$, que en un grafo conexo es único y vale precisamente $1$, mientras que el autovalor asociado es $\\lambda_{\\min}(L)=0$. En consecuencia, se ha mostrado la invertibilidad de $L+\\mu I$ para todo $\\mu>0$, la convergencia del método de la potencia aplicado a su inversa hacia el autovector del autovalor más pequeño, y, cuando dicho autovalor es simple, se ha determinado explícitamente tanto el autovector ($1$) como el valor propio correspondiente ($0$).\n",
    "## Punto c: Deflación de Hotelling\n",
    "Suponiendo que $M$ es simétrica (y por lo tanto admite una base ortogonal de autovectores), muestre que la matriz $\\tilde{M}-\\lambda_1\\frac{v_1v_1^t}{v_1^tv_1}$ tiene los mismos autovectores que $M$, pero el autovalor asociado a $v_1$ es igual a cero.\n",
    "### Respuesta\n",
    "Sea $M\\in\\mathbb{R}^{n\\times n}$ una matriz simétrica. Por el teorema espectral, se sabe que toda matriz simétrica es diagonalizable mediante una base ortonormal de autovectores. Esto significa que existen $n$ vectores $\\{v_1,v_2,\\dots,v_n\\}\\subset\\mathbb{R}^n$, ortogonales entre sí, tales que para cada $i\\in\\{1,\\dots,n\\}$, se cumple M $v_i=\\lambda_iv_i$, donde $\\lambda_i\\in\\mathbb{R}$ es el autovalor correspondiente. Supóngase, sin pérdida de generalidad, que $\\lambda_1$ es el autovalor de mayor módulo, y que $v_1$ es el autovector correspondiente.\n",
    "Con el objetivo de eliminar la contribución del autovalor dominante $\\lambda_1$ del espectro de $M$, se define la matriz modificada\n",
    "$$\\tilde{M}=M-\\lambda_1\\frac{v_1v_1^t}{v_1^tv_1}$$\n",
    "y se desea demostrar que $\\tilde{M}$ conserva los mismos autovectores que $M$, pero que el autovalor asociado a $v_1$ pasa a ser cero.\n",
    "Para proceder con la demostración, se analiza la acción de $\\tilde{M}$ sobre un vector arbitrario $v_i$ de la base ortogonal de autovectores de $M$. Primero se considera el caso en que $v_i=v_1$. Dado que $v_1$ es un autovector de $M$, se cumple $M v_1=\\lambda_1v_1$, y por lo tanto:\n",
    "$$\\tilde{M}v_1=Mv_1-\\lambda_1\\frac{v_1v_1^t}{v_1^t v_1}v_1=\\lambda_1v_1-\\lambda_1\\frac{v_1(v_1^t v_1)}{v_1^t v_1}=\\lambda_1v_1-\\lambda_1v_1=0$$\n",
    "Por consiguiente, $v_1$ sigue siendo autovector de $\\tilde{M}$, pero ahora asociado al autovalor $0$.\n",
    "A continuación, se considera el caso general $v_i$, con $i\\neq1$. Como $\\{v_i\\}$ es una base ortogonal de autovectores de $M$, se cumple $Mv_i=\\lambda_iv_i$, y además $v_1^tv_i=0$. Evaluando $\\tilde{M}v_i$, se tiene:\n",
    "$$\\tilde{M}v_i=Mv_i-\\lambda_1\\frac{v_1 v_1^t}{v_1^t v_1}v_i=\\lambda_iv_i-\\lambda_1\\frac{v_1 (v_1^t v_i)}{v_1^t v_1}=\\lambda_iv_i-\\lambda_1\\cdot\\frac{v_1\\cdot0}{v_1^t v_1}=\\lambda_iv_i$$\n",
    "De este modo, para todo $i\\neq1$, $v_i$ continúa siendo autovector de $\\tilde{M}$, con el mismo autovalor que tenía en $M$. Se concluye entonces que la matriz $\\tilde{M}$ tiene exactamente la misma base ortonormal de autovectores que $M$, pero el autovalor asociado a $v_1$ ha sido reemplazado por cero. El espectro de $\\tilde{M}$ consiste en $\\{0,\\lambda_2,\\dots,\\lambda_n\\}$, respetando las multiplicidades de los autovalores restantes.\n",
    "Este procedimiento se conoce como **deflación de Hotelling**, y constituye una herramienta fundamental para eliminar del análisis espectral la contribución del autovalor dominante, permitiendo aplicar métodos iterativos como la potencia para hallar sucesivamente los autovectores asociados a autovalores secundarios. En el contexto del presente trabajo, esta técnica permite acceder al segundo autovalor más pequeño de la matriz laplaciana $L$, o al segundo autovalor más grande de la matriz de modularidad $R$, al eliminar previamente la influencia del vector trivial asociado a $1$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d31e12f7-40ec-4b57-a8ef-47432cf748c8",
   "metadata": {},
   "source": [
    "# Ejercicio 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6aa745e-460d-4d58-9283-a0174ba44ea9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargamos librerías\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.linalg import solve_triangular\n",
    "\n",
    "# Cargamos funciones auxiliares\n",
    "\n",
    "# Función para permutar filas (para descompPLU)\n",
    "def permutacion(A, vector_P, index):\n",
    "    n = A.shape[0]\n",
    "    max_index = index + np.argmax(np.abs(A[index:, index]))\n",
    "    #swap\n",
    "    if max_index != index:\n",
    "        A[[index, max_index]] = A[[max_index, index]]\n",
    "        vector_P[[index, max_index]] = vector_P[[max_index, index]]\n",
    "\n",
    "\n",
    "# Descomposición PLU con pivoteo\n",
    "def calculaPLU(m, verbose=False):\n",
    "    mc = m.copy().astype(np.float64)\n",
    "    n = m.shape[0]\n",
    "    P = np.eye(n)\n",
    "    for i in range(n - 1):\n",
    "        max_row = i + np.argmax(np.abs(mc[i:, i]))\n",
    "        if max_row != i:\n",
    "            mc[[i, max_row]] = mc[[max_row, i]]\n",
    "            P[[i, max_row]] = P[[max_row, i]]\n",
    "        a_ii = mc[i, i]\n",
    "        if a_ii == 0:\n",
    "            raise ValueError(\"Matriz singular (no invertible)\")\n",
    "        L_i = mc[i+1:, i] / a_ii\n",
    "        mc[i+1:, i] = L_i\n",
    "        mc[i+1:, i+1:] -= np.outer(L_i, mc[i, i+1:])\n",
    "    \n",
    "    L = np.tril(mc, -1) + np.eye(n)\n",
    "    U = np.triu(mc)\n",
    "    if verbose:\n",
    "        print(\"P:\\n\", P)\n",
    "        print(\"L:\\n\", L)\n",
    "        print(\"U:\\n\", U)\n",
    "    return P, L, U\n",
    "\n",
    "\n",
    "def calculaLU(matriz, verbose=False):\n",
    "    mc = matriz.copy().astype(np.float64)\n",
    "    n = matriz.shape[0]\n",
    "    for i in range(n - 1):\n",
    "        a_ii = mc[i, i]\n",
    "        if a_ii == 0:\n",
    "            raise ValueError(\"Cero en la diagonal durante LU (se requiere pivoteo)\")\n",
    "        L_i = mc[i+1:, i] / a_ii\n",
    "        mc[i+1:, i] = L_i\n",
    "        mc[i+1:, i+1:] -= np.outer(L_i, mc[i, i+1:])\n",
    "    \n",
    "    L = np.tril(mc, -1) + np.eye(n)\n",
    "    U = np.triu(mc)\n",
    "    if verbose:\n",
    "        print(\"L:\\n\", L)\n",
    "        print(\"U:\\n\", U)\n",
    "    return L, U\n",
    "\n",
    "# Función para calcular la inversa corregida\n",
    "def inversa(m):\n",
    "    n = m.shape[0]\n",
    "    try:\n",
    "        L, U = calculaLU(m)\n",
    "        P = np.eye(n)  # Matriz de permutación identidad si no hay pivoteo\n",
    "    except (ValueError, np.LinAlgError):\n",
    "        P, L, U = calculaPLU(m)\n",
    "    \n",
    "    m_inv = np.zeros((n, n))\n",
    "    for i in range(n):\n",
    "        e_i = P.T @ np.eye(n)[:, i]  # Aplica la permutación P al vector canónico\n",
    "        y = solve_triangular(L, e_i, lower=True)\n",
    "        x = solve_triangular(U, y, lower=False)\n",
    "        m_inv[:, i] = x\n",
    "    return m_inv\n",
    "\n",
    "def calcula_K (A):\n",
    "    n = A.shape[0]\n",
    "    k = np.zeros((n,n),dtype=A.dtype)\n",
    "    for i in range(n):\n",
    "        k[i][i] = A[i].sum()\n",
    "    \n",
    "    return k\n",
    "\n",
    "def norma2(v):\n",
    "    n = 0\n",
    "    for k in v:\n",
    "        n+=k*k\n",
    "    return np.sqrt(n)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "137b7480-459b-4c26-b9b3-0784129d4597",
   "metadata": {},
   "source": [
    "## a. Construccion de matrices\n",
    "\n",
    "### i. Matrices L y R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1761813f-8714-4c15-8634-83e32bd839fb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c08f74f9-5139-4c3c-b6dc-422d17e3c93a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "271bc17b-1396-4cd2-a9c3-b2ffb63baed6",
   "metadata": {},
   "source": [
    "##  c. Funciones que realizan particiones iterativas de los grafos\n",
    "\n",
    "### i. Funcion para calcular el Laplaciano de forma iterativa (corte mínimo). \n",
    "Sea una red sin dirigir representada en un grafo al cual le asociamos la matriz  $A \\in \\mathbb{R}^{N \\times N}$ para este grafo el cual representa $A_{ij}= 1$ si $i$ y $j$ están conectados y $A_{ij}= 0$ si no.  \n",
    "Podemos detectar _grupos_ en esta red. Por ejemplo para el caso más básico; queremos identificar dos grupos $G_1, G_2$. Representamos la asignación en comunidades mediante el vector **s** definido $s_i =1$ si $i \\in G_1$ y $s_i =-1$ si $i \\in G_2$. Notamos la cantidad de conexiones entre ambos grupos por $\\Lambda$\n",
    "$$\\Lambda = \\frac{1}{4} s^tLs  \\quad \\text{con } L= K -A $$\n",
    "Buscamos un vector $s^{\\Lambda}$ optimo que minimice $\\Lambda$, para ello buscamos el segundo autovector de autovalor más pequeño de la matriz _L_ . Si aplicamos el mètodo de la potencia a $L^{-1}$ el autovalor de menor valor absoluto pasa a ser el dominante. En general, aplicando el método a $(L- \\mu I)^{-1}$ con $\\mu$ cercano a $\\lambda_{N-1}$ conseguiremos que $\\lambda_{N-1}$ sea dominante"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad2ddc69-be5f-4b44-8aea-9bcecb2f788a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddf3e256-5ff2-4180-abd2-64f72e523170",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
